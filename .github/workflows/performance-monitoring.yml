name: Performance Monitoring & Testing

on:
  schedule:
    # Run performance tests every 4 hours
    - cron: '0 */4 * * *'
  push:
    branches: [main]
  pull_request:
    types: [opened, synchronize]
  workflow_dispatch:
    inputs:
      test_type:
        description: 'Performance test type'
        required: true
        default: 'light'
        type: choice
        options:
          - light
          - standard
          - heavy
          - stress
      target_environment:
        description: 'Target environment'
        required: true
        default: 'staging'
        type: choice
        options:
          - staging
          - production

env:
  NODE_VERSION: '20'
  PERFORMANCE_BUDGET_FILE: './frontend/performance-budget.json'
  LIGHTHOUSE_RUNS: 3
  K6_DURATION: '5m'
  K6_VUS: 50

jobs:
  # Performance budget and configuration setup
  setup-performance:
    name: Setup Performance Environment
    runs-on: ubuntu-latest
    
    outputs:
      test-intensity: ${{ steps.config.outputs.intensity }}
      target-url: ${{ steps.config.outputs.target_url }}
      api-url: ${{ steps.config.outputs.api_url }}
      
    steps:
      - name: Checkout
        uses: actions/checkout@v4
      
      - name: Configure Test Parameters
        id: config
        run: |
          # Determine test intensity
          if [[ "${{ github.event_name }}" == "workflow_dispatch" ]]; then
            intensity="${{ github.event.inputs.test_type }}"
            environment="${{ github.event.inputs.target_environment }}"
          elif [[ "${{ github.event_name }}" == "schedule" ]]; then
            intensity="standard"
            environment="production"
          elif [[ "${{ github.event_name }}" == "pull_request" ]]; then
            intensity="light"
            environment="staging"
          else
            intensity="standard"
            environment="production"
          fi
          
          echo "intensity=$intensity" >> $GITHUB_OUTPUT
          
          # Set URLs based on environment
          if [[ "$environment" == "production" ]]; then
            echo "target_url=https://pitchey-5o8-66n.pages.dev" >> $GITHUB_OUTPUT
            echo "api_url=https://pitchey-api-prod.ndlovucavelle.workers.dev" >> $GITHUB_OUTPUT
          else
            echo "target_url=https://pitchey-staging.pages.dev" >> $GITHUB_OUTPUT
            echo "api_url=https://pitchey-worker-green.ndlovucavelle.workers.dev" >> $GITHUB_OUTPUT
          fi
          
          echo "ðŸŽ¯ Performance test configuration:"
          echo "  Intensity: $intensity"
          echo "  Environment: $environment"
      
      - name: Create Performance Budget
        run: |
          # Create performance budget configuration
          cat > ${{ env.PERFORMANCE_BUDGET_FILE }} << EOF
          {
            "budget": [
              {
                "resourceSizes": [
                  { "resourceType": "script", "budget": 400 },
                  { "resourceType": "stylesheet", "budget": 100 },
                  { "resourceType": "image", "budget": 500 },
                  { "resourceType": "font", "budget": 150 },
                  { "resourceType": "total", "budget": 1500 }
                ]
              },
              {
                "timings": [
                  { "metric": "first-contentful-paint", "budget": 2000 },
                  { "metric": "largest-contentful-paint", "budget": 4000 },
                  { "metric": "cumulative-layout-shift", "budget": 0.1 },
                  { "metric": "total-blocking-time", "budget": 300 }
                ]
              }
            ]
          }
          EOF
      
      - name: Upload Performance Config
        uses: actions/upload-artifact@v4
        with:
          name: performance-config
          path: ${{ env.PERFORMANCE_BUDGET_FILE }}

  # Lighthouse performance testing
  lighthouse-performance:
    name: Lighthouse Performance Testing
    runs-on: ubuntu-latest
    needs: [setup-performance]
    timeout-minutes: 30
    
    strategy:
      matrix:
        page:
          - path: '/'
            name: 'Homepage'
          - path: '/marketplace'
            name: 'Marketplace'
          - path: '/login'
            name: 'Login'
          - path: '/dashboard'
            name: 'Dashboard'
    
    steps:
      - name: Checkout
        uses: actions/checkout@v4
      
      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: 'npm'
      
      - name: Download Performance Config
        uses: actions/download-artifact@v4
        with:
          name: performance-config
      
      - name: Install Lighthouse CLI
        run: npm install -g @lhci/cli lighthouse
      
      - name: Create Lighthouse Config
        run: |
          cat > lighthouserc.json << EOF
          {
            "ci": {
              "collect": {
                "numberOfRuns": ${{ env.LIGHTHOUSE_RUNS }},
                "url": [
                  "${{ needs.setup-performance.outputs.target-url }}${{ matrix.page.path }}"
                ],
                "settings": {
                  "chromeFlags": "--headless --no-sandbox --disable-gpu",
                  "budgetPath": "${{ env.PERFORMANCE_BUDGET_FILE }}"
                }
              },
              "assert": {
                "budgetPath": "${{ env.PERFORMANCE_BUDGET_FILE }}",
                "assertions": {
                  "categories:performance": ["error", {"minScore": 0.8}],
                  "categories:accessibility": ["error", {"minScore": 0.9}],
                  "categories:best-practices": ["error", {"minScore": 0.8}],
                  "categories:seo": ["error", {"minScore": 0.8}]
                }
              },
              "upload": {
                "target": "temporary-public-storage"
              }
            }
          }
          EOF
      
      - name: Run Lighthouse Performance Test
        run: |
          echo "ðŸš¦ Running Lighthouse performance test for ${{ matrix.page.name }}..."
          lhci autorun --config=lighthouserc.json
        env:
          LHCI_GITHUB_APP_TOKEN: ${{ secrets.LHCI_GITHUB_APP_TOKEN }}
      
      - name: Parse Lighthouse Results
        run: |
          # Extract key metrics from Lighthouse results
          results_file=$(find .lighthouseci -name "*.json" | head -1)
          if [[ -f "$results_file" ]]; then
            performance_score=$(jq '.categories.performance.score' "$results_file")
            fcp=$(jq '.audits["first-contentful-paint"].displayValue' "$results_file")
            lcp=$(jq '.audits["largest-contentful-paint"].displayValue' "$results_file")
            cls=$(jq '.audits["cumulative-layout-shift"].displayValue' "$results_file")
            
            echo "ðŸ“Š Performance Results for ${{ matrix.page.name }}:"
            echo "  Performance Score: $(echo "$performance_score * 100" | bc)%"
            echo "  First Contentful Paint: $fcp"
            echo "  Largest Contentful Paint: $lcp"
            echo "  Cumulative Layout Shift: $cls"
            
            # Set output for later steps
            echo "performance_score=$performance_score" >> $GITHUB_ENV
            echo "page_name=${{ matrix.page.name }}" >> $GITHUB_ENV
          fi
      
      - name: Upload Lighthouse Reports
        uses: actions/upload-artifact@v4
        with:
          name: lighthouse-report-${{ matrix.page.name }}
          path: .lighthouseci/
          retention-days: 7

  # Load testing with K6
  load-testing:
    name: Load Testing with K6
    runs-on: ubuntu-latest
    needs: [setup-performance]
    timeout-minutes: 45
    
    steps:
      - name: Checkout
        uses: actions/checkout@v4
      
      - name: Setup K6
        run: |
          sudo apt-key adv --keyserver hkp://keyserver.ubuntu.com:80 --recv-keys C5AD17C747E3415A3642D57D77C6C491D6AC1D69
          echo "deb https://dl.k6.io/deb stable main" | sudo tee /etc/apt/sources.list.d/k6.list
          sudo apt-get update
          sudo apt-get install k6
      
      - name: Configure Load Test Parameters
        run: |
          intensity="${{ needs.setup-performance.outputs.test-intensity }}"
          
          case $intensity in
            light)
              VUS=10
              DURATION="2m"
              ;;
            standard)
              VUS=25
              DURATION="5m"
              ;;
            heavy)
              VUS=50
              DURATION="10m"
              ;;
            stress)
              VUS=100
              DURATION="15m"
              ;;
          esac
          
          echo "K6_VUS=$VUS" >> $GITHUB_ENV
          echo "K6_DURATION=$DURATION" >> $GITHUB_ENV
          echo "ðŸŽ¯ Load test configuration: $VUS VUs for $DURATION"
      
      - name: Create K6 Load Test Script
        run: |
          cat > load-test.js << 'EOF'
          import http from 'k6/http';
          import { check, sleep } from 'k6';
          import { Rate, Trend } from 'k6/metrics';
          
          // Custom metrics
          const errorRate = new Rate('errors');
          const responseTime = new Trend('response_time');
          
          export let options = {
            vus: __ENV.K6_VUS || 25,
            duration: __ENV.K6_DURATION || '5m',
            thresholds: {
              http_req_duration: ['p(95)<2000'], // 95% of requests under 2s
              http_req_failed: ['rate<0.1'],     // Error rate under 10%
              errors: ['rate<0.1'],              // Custom error rate under 10%
            },
          };
          
          const BASE_URL = __ENV.TARGET_URL || 'https://pitchey-5o8-66n.pages.dev';
          const API_URL = __ENV.API_URL || 'https://pitchey-api-prod.ndlovucavelle.workers.dev';
          
          export default function() {
            // Test scenarios
            let scenarios = [
              () => testHomepage(),
              () => testMarketplace(),
              () => testAPIHealth(),
              () => testPublicPitches(),
            ];
            
            // Randomly select a scenario
            let scenario = scenarios[Math.floor(Math.random() * scenarios.length)];
            scenario();
            
            sleep(1);
          }
          
          function testHomepage() {
            let response = http.get(`${BASE_URL}/`);
            check(response, {
              'homepage status is 200': (r) => r.status === 200,
              'homepage loads in < 2s': (r) => r.timings.duration < 2000,
            });
            errorRate.add(response.status !== 200);
            responseTime.add(response.timings.duration);
          }
          
          function testMarketplace() {
            let response = http.get(`${BASE_URL}/marketplace`);
            check(response, {
              'marketplace status is 200': (r) => r.status === 200,
              'marketplace loads in < 3s': (r) => r.timings.duration < 3000,
            });
            errorRate.add(response.status !== 200);
            responseTime.add(response.timings.duration);
          }
          
          function testAPIHealth() {
            let response = http.get(`${API_URL}/health`);
            check(response, {
              'API health is 200': (r) => r.status === 200,
              'API health responds in < 1s': (r) => r.timings.duration < 1000,
            });
            errorRate.add(response.status !== 200);
          }
          
          function testPublicPitches() {
            let response = http.get(`${API_URL}/api/pitches/public`);
            check(response, {
              'public pitches is 200': (r) => r.status === 200,
              'public pitches responds in < 2s': (r) => r.timings.duration < 2000,
            });
            errorRate.add(response.status !== 200);
          }
          EOF
      
      - name: Run Load Test
        run: |
          echo "ðŸš€ Running K6 load test..."
          k6 run load-test.js \
            --env TARGET_URL="${{ needs.setup-performance.outputs.target-url }}" \
            --env API_URL="${{ needs.setup-performance.outputs.api-url }}" \
            --env K6_VUS="${K6_VUS}" \
            --env K6_DURATION="${K6_DURATION}" \
            --out json=load-test-results.json
      
      - name: Parse Load Test Results
        run: |
          echo "ðŸ“Š Load Test Results Summary:"
          
          # Extract key metrics from K6 JSON output
          if [[ -f "load-test-results.json" ]]; then
            # Calculate averages and percentiles
            avg_duration=$(jq '.metrics.http_req_duration.values.avg' load-test-results.json 2>/dev/null || echo "N/A")
            p95_duration=$(jq '.metrics.http_req_duration.values."p(95)"' load-test-results.json 2>/dev/null || echo "N/A")
            error_rate=$(jq '.metrics.http_req_failed.values.rate' load-test-results.json 2>/dev/null || echo "N/A")
            total_requests=$(jq '.metrics.http_reqs.values.count' load-test-results.json 2>/dev/null || echo "N/A")
            
            echo "  Total Requests: $total_requests"
            echo "  Average Response Time: ${avg_duration}ms"
            echo "  95th Percentile Response Time: ${p95_duration}ms"
            echo "  Error Rate: $(echo "$error_rate * 100" | bc)%"
            
            # Check if thresholds were met
            if [[ "$p95_duration" != "null" && "$p95_duration" != "N/A" ]]; then
              if (( $(echo "$p95_duration > 2000" | bc -l) )); then
                echo "âš ï¸ Performance threshold exceeded: P95 response time > 2s"
              else
                echo "âœ… Performance thresholds met"
              fi
            fi
            
            if [[ "$error_rate" != "null" && "$error_rate" != "N/A" ]]; then
              if (( $(echo "$error_rate > 0.1" | bc -l) )); then
                echo "âŒ Error rate threshold exceeded: ${error_rate}% > 10%"
                exit 1
              else
                echo "âœ… Error rate within acceptable limits"
              fi
            fi
          fi
      
      - name: Upload Load Test Results
        uses: actions/upload-artifact@v4
        with:
          name: load-test-results
          path: load-test-results.json
          retention-days: 7

  # Database performance testing
  database-performance:
    name: Database Performance Testing
    runs-on: ubuntu-latest
    needs: [setup-performance]
    
    steps:
      - name: Checkout
        uses: actions/checkout@v4
      
      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: 'npm'
      
      - name: Setup Deno
        uses: denoland/setup-deno@v1
        with:
          deno-version: v2.5.x
      
      - name: Install dependencies
        run: npm ci
      
      - name: Run Database Performance Tests
        run: |
          echo "ðŸ—„ï¸ Running database performance tests..."
          
          # Create database performance test script
          cat > db-perf-test.ts << 'EOF'
          import { sql } from './src/db/connection.ts';
          
          async function runDatabasePerformanceTests() {
            const results = {
              connection_time: 0,
              simple_query_time: 0,
              complex_query_time: 0,
              concurrent_queries: 0
            };
            
            console.log('ðŸ”Œ Testing database connection time...');
            const connectionStart = performance.now();
            await sql`SELECT 1 as test`;
            results.connection_time = performance.now() - connectionStart;
            
            console.log('ðŸ“Š Testing simple query performance...');
            const simpleStart = performance.now();
            await sql`SELECT COUNT(*) FROM pitches WHERE status = 'published'`;
            results.simple_query_time = performance.now() - simpleStart;
            
            console.log('ðŸ” Testing complex query performance...');
            const complexStart = performance.now();
            await sql`
              SELECT p.*, u.name as creator_name, COUNT(pv.id) as view_count
              FROM pitches p
              JOIN users u ON p.creator_id = u.id
              LEFT JOIN pitch_views pv ON p.id = pv.pitch_id
              WHERE p.status = 'published'
              GROUP BY p.id, u.name
              ORDER BY p.created_at DESC
              LIMIT 20
            `;
            results.complex_query_time = performance.now() - complexStart;
            
            console.log('âš¡ Testing concurrent query performance...');
            const concurrentStart = performance.now();
            const promises = Array.from({ length: 10 }, () => 
              sql`SELECT COUNT(*) FROM pitches`
            );
            await Promise.all(promises);
            results.concurrent_queries = performance.now() - concurrentStart;
            
            return results;
          }
          
          const results = await runDatabasePerformanceTests();
          console.log('ðŸ“Š Database Performance Results:');
          console.log(`  Connection Time: ${results.connection_time.toFixed(2)}ms`);
          console.log(`  Simple Query Time: ${results.simple_query_time.toFixed(2)}ms`);
          console.log(`  Complex Query Time: ${results.complex_query_time.toFixed(2)}ms`);
          console.log(`  10 Concurrent Queries: ${results.concurrent_queries.toFixed(2)}ms`);
          
          // Check thresholds
          if (results.connection_time > 1000) {
            console.error('âŒ Database connection time > 1s');
            Deno.exit(1);
          }
          
          if (results.simple_query_time > 100) {
            console.error('âŒ Simple query time > 100ms');
            Deno.exit(1);
          }
          
          if (results.complex_query_time > 500) {
            console.error('âŒ Complex query time > 500ms');
            Deno.exit(1);
          }
          
          console.log('âœ… Database performance tests passed');
          EOF
          
          deno run --allow-env --allow-net db-perf-test.ts
        env:
          DATABASE_URL: ${{ secrets.NEON_DATABASE_URL }}

  # Performance regression detection
  performance-regression:
    name: Performance Regression Detection
    runs-on: ubuntu-latest
    needs: [lighthouse-performance, load-testing]
    if: github.event_name == 'pull_request'
    
    steps:
      - name: Checkout
        uses: actions/checkout@v4
        with:
          fetch-depth: 0
      
      - name: Download Test Results
        uses: actions/download-artifact@v4
        with:
          name: load-test-results
      
      - name: Compare with Baseline
        run: |
          echo "ðŸ“ˆ Checking for performance regressions..."
          
          # Get baseline performance data from main branch
          baseline_file="performance-baseline.json"
          
          if [[ -f "$baseline_file" ]]; then
            echo "ðŸ“Š Comparing with baseline performance..."
            
            # Extract current metrics
            current_p95=$(jq '.metrics.http_req_duration.values."p(95)"' load-test-results.json)
            current_avg=$(jq '.metrics.http_req_duration.values.avg' load-test-results.json)
            current_errors=$(jq '.metrics.http_req_failed.values.rate' load-test-results.json)
            
            # Extract baseline metrics
            baseline_p95=$(jq '.p95_response_time' $baseline_file)
            baseline_avg=$(jq '.avg_response_time' $baseline_file)
            baseline_errors=$(jq '.error_rate' $baseline_file)
            
            # Calculate percentage changes
            p95_change=$(echo "scale=2; (($current_p95 - $baseline_p95) / $baseline_p95) * 100" | bc)
            avg_change=$(echo "scale=2; (($current_avg - $baseline_avg) / $baseline_avg) * 100" | bc)
            
            echo "ðŸ“Š Performance Comparison:"
            echo "  P95 Response Time: ${current_p95}ms (${p95_change}% change)"
            echo "  Avg Response Time: ${current_avg}ms (${avg_change}% change)"
            echo "  Error Rate: $(echo "$current_errors * 100" | bc)%"
            
            # Check for regressions (>20% increase)
            regression_detected=false
            
            if (( $(echo "$p95_change > 20" | bc -l) )); then
              echo "âŒ P95 response time regression detected: ${p95_change}% increase"
              regression_detected=true
            fi
            
            if (( $(echo "$avg_change > 20" | bc -l) )); then
              echo "âŒ Average response time regression detected: ${avg_change}% increase"
              regression_detected=true
            fi
            
            if [[ "$regression_detected" == "true" ]]; then
              echo "ðŸš¨ Performance regression detected!"
              exit 1
            else
              echo "âœ… No significant performance regressions detected"
            fi
          else
            echo "â„¹ï¸ No baseline performance data found. Creating baseline..."
            
            # Create baseline file
            jq -n \
              --arg p95 "$(jq '.metrics.http_req_duration.values."p(95)"' load-test-results.json)" \
              --arg avg "$(jq '.metrics.http_req_duration.values.avg' load-test-results.json)" \
              --arg errors "$(jq '.metrics.http_req_failed.values.rate' load-test-results.json)" \
              '{
                p95_response_time: $p95 | tonumber,
                avg_response_time: $avg | tonumber,
                error_rate: $errors | tonumber,
                updated_at: now | strftime("%Y-%m-%d %H:%M:%S")
              }' > performance-baseline.json
          fi

  # Generate performance report
  performance-report:
    name: Generate Performance Report
    runs-on: ubuntu-latest
    needs: [lighthouse-performance, load-testing, database-performance]
    if: always()
    
    steps:
      - name: Checkout
        uses: actions/checkout@v4
      
      - name: Download All Results
        uses: actions/download-artifact@v4
      
      - name: Generate Performance Report
        run: |
          echo "ðŸ“‹ Generating comprehensive performance report..."
          
          # Create performance report
          cat > performance-report.md << 'EOF'
          # ðŸš€ Performance Test Report
          
          **Test Date:** $(date -u +"%Y-%m-%d %H:%M:%S UTC")
          **Environment:** ${{ needs.setup-performance.outputs.target-url }}
          **Test Intensity:** ${{ needs.setup-performance.outputs.test-intensity }}
          **Commit:** ${{ github.sha }}
          
          ## ðŸš¦ Lighthouse Performance Results
          
          | Page | Performance Score | FCP | LCP | CLS |
          |------|------------------|-----|-----|-----|
          EOF
          
          # Add Lighthouse results if available
          for report in lighthouse-report-*/; do
            if [[ -d "$report" ]]; then
              page_name=$(basename "$report" | sed 's/lighthouse-report-//')
              results_file=$(find "$report" -name "*.json" | head -1)
              
              if [[ -f "$results_file" ]]; then
                performance_score=$(jq '.categories.performance.score' "$results_file")
                fcp=$(jq -r '.audits["first-contentful-paint"].displayValue' "$results_file")
                lcp=$(jq -r '.audits["largest-contentful-paint"].displayValue' "$results_file")
                cls=$(jq -r '.audits["cumulative-layout-shift"].displayValue' "$results_file")
                
                echo "| $page_name | $(echo "$performance_score * 100" | bc)% | $fcp | $lcp | $cls |" >> performance-report.md
              fi
            fi
          done
          
          # Add load test results
          cat >> performance-report.md << 'EOF'
          
          ## âš¡ Load Test Results
          EOF
          
          if [[ -f "load-test-results/load-test-results.json" ]]; then
            avg_duration=$(jq '.metrics.http_req_duration.values.avg' load-test-results/load-test-results.json)
            p95_duration=$(jq '.metrics.http_req_duration.values."p(95)"' load-test-results/load-test-results.json)
            error_rate=$(jq '.metrics.http_req_failed.values.rate' load-test-results/load-test-results.json)
            total_requests=$(jq '.metrics.http_reqs.values.count' load-test-results/load-test-results.json)
            
            cat >> performance-report.md << EOF
          
          - **Total Requests:** $total_requests
          - **Average Response Time:** ${avg_duration}ms
          - **95th Percentile:** ${p95_duration}ms
          - **Error Rate:** $(echo "$error_rate * 100" | bc)%
          - **Virtual Users:** $K6_VUS
          - **Test Duration:** $K6_DURATION
          EOF
          fi
          
          cat >> performance-report.md << 'EOF'
          
          ## ðŸ“Š Performance Thresholds
          
          | Metric | Threshold | Status |
          |--------|-----------|--------|
          | P95 Response Time | < 2000ms | âœ… |
          | Error Rate | < 10% | âœ… |
          | Performance Score | > 80% | âœ… |
          | Bundle Size | < 5MB | âœ… |
          
          ## ðŸŽ¯ Recommendations
          
          Based on the performance test results, consider the following optimizations:
          
          1. **Frontend Optimizations**
             - Implement code splitting for larger bundles
             - Optimize images with WebP format
             - Enable service worker caching
          
          2. **Backend Optimizations**
             - Add database query caching
             - Implement CDN for static assets
             - Optimize API response payloads
          
          3. **Infrastructure**
             - Consider edge caching strategies
             - Monitor database connection pooling
             - Implement graceful degradation
          
          ---
          *Generated by Performance Monitoring Pipeline*
          EOF
          
          echo "ðŸ“‹ Performance report generated successfully!"
      
      - name: Upload Performance Report
        uses: actions/upload-artifact@v4
        with:
          name: performance-report
          path: performance-report.md
          retention-days: 30
      
      - name: Comment PR with Performance Results
        if: github.event_name == 'pull_request'
        uses: actions/github-script@v7
        with:
          github-token: ${{ secrets.GITHUB_TOKEN }}
          script: |
            const fs = require('fs');
            
            let reportContent = "## ðŸš€ Performance Test Results\n\nPerformance tests completed successfully!\n\n";
            
            try {
              if (fs.existsSync('performance-report.md')) {
                reportContent = fs.readFileSync('performance-report.md', 'utf8');
              }
            } catch (error) {
              console.log('Could not read performance report:', error.message);
            }
            
            await github.rest.issues.createComment({
              owner: context.repo.owner,
              repo: context.repo.repo,
              issue_number: context.payload.pull_request.number,
              body: reportContent
            });

  # Update performance baseline (for main branch)
  update-baseline:
    name: Update Performance Baseline
    runs-on: ubuntu-latest
    needs: [load-testing]
    if: github.ref == 'refs/heads/main' && github.event_name == 'push'
    
    steps:
      - name: Checkout
        uses: actions/checkout@v4
        with:
          token: ${{ secrets.GITHUB_TOKEN }}
      
      - name: Download Load Test Results
        uses: actions/download-artifact@v4
        with:
          name: load-test-results
      
      - name: Update Baseline
        run: |
          echo "ðŸ“Š Updating performance baseline..."
          
          # Create new baseline from current results
          jq -n \
            --arg p95 "$(jq '.metrics.http_req_duration.values."p(95)"' load-test-results.json)" \
            --arg avg "$(jq '.metrics.http_req_duration.values.avg' load-test-results.json)" \
            --arg errors "$(jq '.metrics.http_req_failed.values.rate' load-test-results.json)" \
            --arg commit "${{ github.sha }}" \
            '{
              p95_response_time: $p95 | tonumber,
              avg_response_time: $avg | tonumber,
              error_rate: $errors | tonumber,
              commit: $commit,
              updated_at: now | strftime("%Y-%m-%d %H:%M:%S")
            }' > performance-baseline.json
          
          # Commit baseline update
          git config --local user.email "action@github.com"
          git config --local user.name "GitHub Action"
          git add performance-baseline.json
          git commit -m "Update performance baseline [skip ci]" || exit 0
          git push