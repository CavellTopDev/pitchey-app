/**\n * Cache Integration Patch for worker-production-db.ts\n * This patch adds advanced cache warming and management to the existing worker\n */\n\n// Add these imports at the top after existing imports\nimport {\n  cacheMiddleware,\n  initializeCacheSystem,\n  getCacheHealthStatus,\n  shouldCacheRequest,\n  getCacheKey,\n  createCachedResponse,\n  setCachedResponse,\n  getCachedResponse,\n  isCacheManagementEndpoint,\n  handleCacheManagement\n} from './cache/worker-cache-integration';\n\n// Add this global variable after existing globals\nlet cacheSystemInitialized = false;\n\n// Replace the existing cache logic in the fetch handler\n// Around line 1040-1055, replace the EdgeCache logic with:\n\n/*\n// Initialize cache system on first request\nif (!cacheSystemInitialized && env.KV) {\n  try {\n    ctx.waitUntil(\n      initializeCacheSystem(env, ctx).then(() => {\n        cacheSystemInitialized = true;\n        console.log('âœ… Advanced cache system initialized on worker startup');\n      }).catch(error => {\n        console.error('âŒ Cache system initialization failed:', error);\n      })\n    );\n  } catch (error) {\n    console.error('Failed to start cache initialization:', error);\n  }\n}\n\n// Handle cache management endpoints\nif (isCacheManagementEndpoint(path)) {\n  return handleCacheManagement(request, env);\n}\n\n// Try to get cached response for cacheable GET requests\nif (shouldCacheRequest(request)) {\n  try {\n    const endpoint = path.replace('/api/', '');\n    const params = Object.fromEntries(url.searchParams);\n    \n    const cacheResult = await getCachedResponse(\n      endpoint,\n      async () => {\n        // This function will only be called if cache miss\n        // We'll return null here and let the normal request flow handle it\n        return null;\n      },\n      {\n        params,\n        skipCache: false,\n        fallbackToMemory: true\n      }\n    );\n    \n    if (cacheResult !== null) {\n      console.log(`ðŸŽ¯ Cache HIT for ${endpoint}`);\n      const hitResponse = createCachedResponse(cacheResult, request);\n      return perf.wrapResponse(hitResponse, 'HIT');\n    }\n    \n    console.log(`ðŸ’¨ Cache MISS for ${endpoint}`);\n    \n  } catch (error) {\n    console.error('Cache lookup error:', error);\n    // Continue with normal flow\n  }\n}\n*/\n\n// Add this helper function after the existing helper functions (around line 928)\n\n/**\n * Cache response data after successful API call\n */\nasync function cacheResponseData(\n  endpoint: string,\n  data: any,\n  ctx: ExecutionContext,\n  params?: Record<string, any>,\n  ttl?: number\n): Promise<void> {\n  try {\n    ctx.waitUntil(\n      setCachedResponse(endpoint, data, { params, ttl })\n        .then(success => {\n          if (success) {\n            console.log(`ðŸ“¦ Cached response for ${endpoint}`);\n          }\n        })\n        .catch(error => {\n          console.error(`Failed to cache ${endpoint}:`, error);\n        })\n    );\n  } catch (error) {\n    console.error(`Cache storage error for ${endpoint}:`, error);\n  }\n}\n\n// Add cache headers to successful API responses\n// In the corsResponse function, add cache headers for cacheable responses:\n\n/*\n// Add after line where headers are set:\nif (shouldCacheRequest(request) && data?.success === true) {\n  responseHeaders['X-Cache'] = 'MISS';\n  responseHeaders['X-Cache-Source'] = 'pitchey-advanced-cache';\n  \n  // Cache the response asynchronously\n  const url = new URL(request.url);\n  const endpoint = url.pathname.replace('/api/', '');\n  const params = Object.fromEntries(url.searchParams);\n  \n  cacheResponseData(endpoint, data, ctx, params);\n}\n*/\n\n// Add cache warming endpoints to the health check response\n// Around line 955-960, add cache status:\n\n/*\nconst cacheHealth = await getCacheHealthStatus();\n\nreturn new Response(\n  JSON.stringify({\n    status: 'ok',\n    timestamp: new Date().toISOString(),\n    services: {\n      database: dbHealthy,\n      cache: {\n        available: !!env.KV,\n        healthy: cacheHealth.healthy,\n        hitRate: cacheHealth.hitRate || 0,\n        totalRequests: cacheHealth.totalRequests || 0\n      },\n      worker: true\n    },\n    cache: cacheHealth\n  }),\n*/\n\n// Usage Instructions:\n// 1. Add the imports at the top of worker-production-db.ts\n// 2. Replace the existing EdgeCache logic with the new cache middleware\n// 3. Add cache response headers to successful API responses\n// 4. Update health check to include cache status\n// 5. The cache system will automatically:\n//    - Initialize on first request\n//    - Warm cache with critical endpoints\n//    - Monitor and optimize cache performance\n//    - Provide cache management endpoints\n//    - Track hit rates and performance metrics\n\n// Testing:\n// 1. Deploy the updated worker\n// 2. Run: node scripts/test-cache-performance.js\n// 3. Check: GET /api/cache/health\n// 4. Monitor: GET /api/cache/metrics\n// 5. Benchmark: POST /api/cache/benchmark\n\n/**\n * Integration Notes:\n * \n * 1. The advanced cache system provides:\n *    - Intelligent cache warming on startup\n *    - Dynamic TTL based on content type\n *    - Real-time hit rate monitoring\n *    - Automatic optimization\n *    - Fallback memory cache\n *    - Comprehensive analytics\n * \n * 2. Cache Management Endpoints:\n *    GET  /api/cache/health      - Cache health status\n *    GET  /api/cache/metrics     - Detailed metrics\n *    GET  /api/cache/insights    - Optimization insights\n *    POST /api/cache/warm        - Trigger warming\n *    POST /api/cache/optimize    - Trigger optimization\n *    POST /api/cache/invalidate  - Invalidate cache\n *    POST /api/cache/benchmark   - Run performance test\n * \n * 3. Expected Performance:\n *    - >80% cache hit rate for frequent endpoints\n *    - <100ms average cache latency\n *    - Automatic warming every 5 minutes\n *    - Optimization every 15 minutes\n * \n * 4. Monitoring:\n *    - Real-time hit rate tracking\n *    - Performance anomaly detection\n *    - Automatic recommendations\n *    - Health scoring and alerts\n */"