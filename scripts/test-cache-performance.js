#!/usr/bin/env node\n/**\n * Cache Performance Test Script\n * Tests Pitchey cache system to validate >80% hit rate achievement\n * \n * Usage: node scripts/test-cache-performance.js [options]\n */\n\nconst https = require('https');\nconst http = require('http');\n\n// Configuration\nconst config = {\n  // Test against production worker or local dev\n  baseUrl: process.env.TEST_URL || 'https://pitchey-production.cavelltheleaddev.workers.dev',\n  // baseUrl: 'http://localhost:8787', // For local testing\n  \n  // Test parameters\n  warmCacheFirst: true,\n  testIterations: 50,\n  concurrentRequests: 5,\n  testDurationMs: 120000, // 2 minutes\n  targetHitRate: 80, // 80% target\n  \n  // Test endpoints\n  endpoints: [\n    '/api/pitches/browse/enhanced',\n    '/api/pitches/trending?limit=10',\n    '/api/pitches/new?limit=10', \n    '/api/dashboard/stats',\n    '/api/config/app',\n    '/api/pitches/browse/general?page=1&limit=20'\n  ],\n  \n  // Cache management endpoints\n  cacheEndpoints: {\n    health: '/api/cache/health',\n    metrics: '/api/cache/metrics',\n    warm: '/api/cache/warm',\n    benchmark: '/api/cache/benchmark',\n    optimize: '/api/cache/optimize'\n  }\n};\n\n// Helper function to make HTTP requests\nfunction makeRequest(url, options = {}) {\n  return new Promise((resolve, reject) => {\n    const urlObj = new URL(url);\n    const client = urlObj.protocol === 'https:' ? https : http;\n    \n    const requestOptions = {\n      hostname: urlObj.hostname,\n      port: urlObj.port,\n      path: urlObj.pathname + urlObj.search,\n      method: options.method || 'GET',\n      headers: {\n        'Content-Type': 'application/json',\n        'User-Agent': 'Cache-Performance-Test/1.0',\n        ...options.headers\n      }\n    };\n    \n    const req = client.request(requestOptions, (res) => {\n      let data = '';\n      \n      res.on('data', chunk => {\n        data += chunk;\n      });\n      \n      res.on('end', () => {\n        try {\n          const result = {\n            statusCode: res.statusCode,\n            headers: res.headers,\n            data: data ? JSON.parse(data) : null\n          };\n          resolve(result);\n        } catch (error) {\n          resolve({\n            statusCode: res.statusCode,\n            headers: res.headers,\n            data: data,\n            parseError: error.message\n          });\n        }\n      });\n    });\n    \n    req.on('error', reject);\n    \n    if (options.body) {\n      req.write(typeof options.body === 'string' ? options.body : JSON.stringify(options.body));\n    }\n    \n    req.end();\n  });\n}\n\n// Test cache health\nasync function testCacheHealth() {\n  console.log('\\nüîç Testing cache health...');\n  \n  try {\n    const response = await makeRequest(`${config.baseUrl}${config.cacheEndpoints.health}`);\n    \n    if (response.statusCode === 200 && response.data?.success) {\n      const health = response.data.data;\n      console.log(`‚úÖ Cache system is healthy`);\n      console.log(`   Hit rate: ${health.hitRate}%`);\n      console.log(`   Average latency: ${health.averageLatency}ms`);\n      console.log(`   Total requests: ${health.totalRequests}`);\n      \n      if (health.alerts && health.alerts.length > 0) {\n        console.log(`‚ö†Ô∏è  Alerts: ${health.alerts.join(', ')}`);\n      }\n      \n      return health;\n    } else {\n      console.log(`‚ùå Cache health check failed: ${response.statusCode}`);\n      return null;\n    }\n  } catch (error) {\n    console.log(`‚ùå Cache health check error: ${error.message}`);\n    return null;\n  }\n}\n\n// Warm the cache\nasync function warmCache() {\n  console.log('\\nüî• Warming cache...');\n  \n  try {\n    const response = await makeRequest(`${config.baseUrl}${config.cacheEndpoints.warm}`, {\n      method: 'POST',\n      body: {}\n    });\n    \n    if (response.statusCode === 200 && response.data?.success) {\n      const result = response.data.data;\n      console.log(`‚úÖ Cache warming completed`);\n      console.log(`   Success: ${result.success} endpoints`);\n      console.log(`   Failed: ${result.failed} endpoints`);\n      console.log(`   Duration: ${result.duration}ms`);\n      \n      return true;\n    } else {\n      console.log(`‚ùå Cache warming failed: ${response.statusCode}`);\n      return false;\n    }\n  } catch (error) {\n    console.log(`‚ùå Cache warming error: ${error.message}`);\n    return false;\n  }\n}\n\n// Test individual endpoint performance\nasync function testEndpointPerformance(endpoint, iterations = 10) {\n  console.log(`\\nüìä Testing endpoint: ${endpoint}`);\n  \n  const results = {\n    endpoint,\n    iterations,\n    requests: 0,\n    successes: 0,\n    failures: 0,\n    cacheHits: 0,\n    cacheMisses: 0,\n    totalLatency: 0,\n    minLatency: Infinity,\n    maxLatency: 0,\n    errors: []\n  };\n  \n  for (let i = 0; i < iterations; i++) {\n    const startTime = Date.now();\n    \n    try {\n      const response = await makeRequest(`${config.baseUrl}${endpoint}`);\n      const latency = Date.now() - startTime;\n      \n      results.requests++;\n      results.totalLatency += latency;\n      results.minLatency = Math.min(results.minLatency, latency);\n      results.maxLatency = Math.max(results.maxLatency, latency);\n      \n      if (response.statusCode >= 200 && response.statusCode < 300) {\n        results.successes++;\n        \n        // Check cache headers to determine if it was a cache hit\n        const cacheControl = response.headers['cache-control'];\n        const cfCacheStatus = response.headers['cf-cache-status'];\n        const age = response.headers['age'];\n        \n        // These are heuristics - actual cache hit detection may vary\n        if (cfCacheStatus === 'HIT' || age > 0 || latency < 50) {\n          results.cacheHits++;\n        } else {\n          results.cacheMisses++;\n        }\n      } else {\n        results.failures++;\n        results.errors.push(`HTTP ${response.statusCode}`);\n      }\n      \n    } catch (error) {\n      results.failures++;\n      results.errors.push(error.message);\n      results.totalLatency += Date.now() - startTime;\n    }\n    \n    // Small delay between requests\n    if (i < iterations - 1) {\n      await new Promise(resolve => setTimeout(resolve, 100));\n    }\n  }\n  \n  results.averageLatency = results.requests > 0 ? results.totalLatency / results.requests : 0;\n  results.successRate = results.requests > 0 ? (results.successes / results.requests) * 100 : 0;\n  results.estimatedHitRate = results.requests > 0 ? (results.cacheHits / results.requests) * 100 : 0;\n  \n  console.log(`   Requests: ${results.requests}, Success: ${results.successRate.toFixed(1)}%`);\n  console.log(`   Avg latency: ${results.averageLatency.toFixed(1)}ms (min: ${results.minLatency}ms, max: ${results.maxLatency}ms)`);\n  console.log(`   Estimated hit rate: ${results.estimatedHitRate.toFixed(1)}%`);\n  \n  if (results.errors.length > 0) {\n    console.log(`   Errors: ${results.errors.slice(0, 3).join(', ')}${results.errors.length > 3 ? '...' : ''}`);\n  }\n  \n  return results;\n}\n\n// Run comprehensive cache benchmark\nasync function runCacheBenchmark() {\n  console.log('\\nüèÅ Running comprehensive cache benchmark...');\n  \n  try {\n    const response = await makeRequest(`${config.baseUrl}${config.cacheEndpoints.benchmark}`, {\n      method: 'POST',\n      body: {\n        endpoints: config.endpoints,\n        iterations: Math.min(config.testIterations, 30), // Limit for benchmark API\n        warmCache: config.warmCacheFirst,\n        testDuration: 60000 // 1 minute for API benchmark\n      }\n    });\n    \n    if (response.statusCode === 200 && response.data?.success) {\n      const benchmark = response.data.data;\n      \n      console.log(`‚úÖ Benchmark completed`);\n      console.log(`   Overall hit rate: ${benchmark.summary.overallHitRate.toFixed(1)}%`);\n      console.log(`   Total requests: ${benchmark.summary.totalRequests}`);\n      console.log(`   Average latency: ${benchmark.summary.averageLatency.toFixed(1)}ms`);\n      console.log(`   Test duration: ${benchmark.summary.testDurationActual}ms`);\n      \n      console.log('\\nüìà Endpoint breakdown:');\n      benchmark.results.forEach(result => {\n        console.log(`   ${result.endpoint}: ${result.hitRate.toFixed(1)}% hit rate, ${result.averageLatency.toFixed(1)}ms avg`);\n      });\n      \n      return benchmark;\n    } else {\n      console.log(`‚ùå Benchmark failed: ${response.statusCode}`);\n      console.log(`   Response: ${JSON.stringify(response.data)}`);\n      return null;\n    }\n  } catch (error) {\n    console.log(`‚ùå Benchmark error: ${error.message}`);\n    return null;\n  }\n}\n\n// Trigger cache optimization\nasync function optimizeCache() {\n  console.log('\\n‚ö° Optimizing cache performance...');\n  \n  try {\n    const response = await makeRequest(`${config.baseUrl}${config.cacheEndpoints.optimize}`, {\n      method: 'POST',\n      body: {}\n    });\n    \n    if (response.statusCode === 200 && response.data?.success) {\n      const result = response.data.data;\n      \n      if (result) {\n        console.log(`‚úÖ Cache optimization completed`);\n        console.log(`   Rules applied: ${result.rulesApplied.join(', ')}`);\n        console.log(`   Hit rate improvement: ${result.improvementPercent > 0 ? '+' : ''}${result.improvementPercent.toFixed(1)}%`);\n        console.log(`   Before: ${result.metricsBeforeOptimization.hitRate.toFixed(1)}% hit rate`);\n        console.log(`   After: ${result.metricsAfterOptimization.hitRate.toFixed(1)}% hit rate`);\n        \n        if (result.recommendations.length > 0) {\n          console.log('   Recommendations:');\n          result.recommendations.forEach(rec => {\n            console.log(`     ‚Ä¢ ${rec}`);\n          });\n        }\n      } else {\n        console.log(`‚úÖ No optimization needed - cache performance is good`);\n      }\n      \n      return result;\n    } else {\n      console.log(`‚ùå Cache optimization failed: ${response.statusCode}`);\n      return null;\n    }\n  } catch (error) {\n    console.log(`‚ùå Cache optimization error: ${error.message}`);\n    return null;\n  }\n}\n\n// Get cache metrics\nasync function getCacheMetrics() {\n  console.log('\\nüìä Fetching cache metrics...');\n  \n  try {\n    const response = await makeRequest(`${config.baseUrl}${config.cacheEndpoints.metrics}`);\n    \n    if (response.statusCode === 200 && response.data?.success) {\n      const metrics = response.data.data;\n      \n      console.log(`‚úÖ Cache metrics retrieved`);\n      console.log(`   Hit rate: ${metrics.cacheMetrics.hitRate.toFixed(1)}%`);\n      console.log(`   Total requests: ${metrics.cacheMetrics.totalRequests}`);\n      console.log(`   Average latency: ${metrics.cacheMetrics.averageLatency.toFixed(1)}ms`);\n      console.log(`   Fallback cache: ${metrics.fallbackStats.entries} entries, ${(metrics.fallbackStats.totalSizeBytes / 1024).toFixed(1)}KB`);\n      \n      const health = metrics.optimizerDiagnostics.performanceHealth;\n      console.log(`   Performance score: ${health.score}/100 (${health.status})`);\n      \n      if (health.keyIssues.length > 0) {\n        console.log(`   Issues: ${health.keyIssues.join(', ')}`);\n      }\n      \n      return metrics;\n    } else {\n      console.log(`‚ùå Failed to get cache metrics: ${response.statusCode}`);\n      return null;\n    }\n  } catch (error) {\n    console.log(`‚ùå Cache metrics error: ${error.message}`);\n    return null;\n  }\n}\n\n// Load test with concurrent requests\nasync function runLoadTest() {\n  console.log(`\\nüöÄ Running load test (${config.concurrentRequests} concurrent requests for ${config.testDurationMs/1000}s)...`);\n  \n  const startTime = Date.now();\n  const results = {\n    totalRequests: 0,\n    successfulRequests: 0,\n    failedRequests: 0,\n    totalLatency: 0,\n    minLatency: Infinity,\n    maxLatency: 0,\n    estimatedCacheHits: 0,\n    endpointStats: new Map()\n  };\n  \n  const workers = [];\n  \n  // Start concurrent workers\n  for (let i = 0; i < config.concurrentRequests; i++) {\n    const worker = (async () => {\n      const workerResults = {\n        requests: 0,\n        successes: 0,\n        failures: 0,\n        latency: 0\n      };\n      \n      while (Date.now() - startTime < config.testDurationMs) {\n        const endpoint = config.endpoints[Math.floor(Math.random() * config.endpoints.length)];\n        const requestStart = Date.now();\n        \n        try {\n          const response = await makeRequest(`${config.baseUrl}${endpoint}`);\n          const latency = Date.now() - requestStart;\n          \n          workerResults.requests++;\n          workerResults.latency += latency;\n          \n          if (response.statusCode >= 200 && response.statusCode < 300) {\n            workerResults.successes++;\n            \n            // Estimate cache hit based on response time\n            if (latency < 100) {\n              results.estimatedCacheHits++;\n            }\n          } else {\n            workerResults.failures++;\n          }\n          \n          results.totalLatency += latency;\n          results.minLatency = Math.min(results.minLatency, latency);\n          results.maxLatency = Math.max(results.maxLatency, latency);\n          \n          // Track per-endpoint stats\n          const endpointStat = results.endpointStats.get(endpoint) || { requests: 0, latency: 0 };\n          endpointStat.requests++;\n          endpointStat.latency += latency;\n          results.endpointStats.set(endpoint, endpointStat);\n          \n        } catch (error) {\n          workerResults.failures++;\n          workerResults.requests++;\n        }\n        \n        // Small delay between requests from same worker\n        await new Promise(resolve => setTimeout(resolve, 50));\n      }\n      \n      return workerResults;\n    })();\n    \n    workers.push(worker);\n  }\n  \n  // Wait for all workers to complete\n  const workerResults = await Promise.all(workers);\n  \n  // Aggregate results\n  workerResults.forEach(wr => {\n    results.totalRequests += wr.requests;\n    results.successfulRequests += wr.successes;\n    results.failedRequests += wr.failures;\n  });\n  \n  const duration = Date.now() - startTime;\n  const averageLatency = results.totalRequests > 0 ? results.totalLatency / results.totalRequests : 0;\n  const successRate = results.totalRequests > 0 ? (results.successfulRequests / results.totalRequests) * 100 : 0;\n  const estimatedHitRate = results.totalRequests > 0 ? (results.estimatedCacheHits / results.totalRequests) * 100 : 0;\n  const requestsPerSecond = results.totalRequests / (duration / 1000);\n  \n  console.log(`‚úÖ Load test completed (${duration}ms)`);\n  console.log(`   Total requests: ${results.totalRequests}`);\n  console.log(`   Success rate: ${successRate.toFixed(1)}%`);\n  console.log(`   Requests/second: ${requestsPerSecond.toFixed(1)}`);\n  console.log(`   Average latency: ${averageLatency.toFixed(1)}ms`);\n  console.log(`   Latency range: ${results.minLatency}ms - ${results.maxLatency}ms`);\n  console.log(`   Estimated cache hit rate: ${estimatedHitRate.toFixed(1)}%`);\n  \n  console.log('\\nüìà Endpoint performance:');\n  results.endpointStats.forEach((stat, endpoint) => {\n    const avgLatency = stat.requests > 0 ? stat.latency / stat.requests : 0;\n    console.log(`   ${endpoint}: ${stat.requests} requests, ${avgLatency.toFixed(1)}ms avg`);\n  });\n  \n  return {\n    ...results,\n    duration,\n    averageLatency,\n    successRate,\n    estimatedHitRate,\n    requestsPerSecond\n  };\n}\n\n// Main test function\nasync function main() {\n  console.log('üß™ Pitchey Cache Performance Test');\n  console.log('=====================================');\n  console.log(`Target: ${config.baseUrl}`);\n  console.log(`Target hit rate: ${config.targetHitRate}%`);\n  console.log(`Test iterations: ${config.testIterations}`);\n  console.log(`Concurrent requests: ${config.concurrentRequests}`);\n  console.log(`Test duration: ${config.testDurationMs/1000}s`);\n  \n  let overallSuccess = true;\n  const testResults = {};\n  \n  try {\n    // 1. Test initial cache health\n    testResults.initialHealth = await testCacheHealth();\n    \n    // 2. Warm cache if enabled\n    if (config.warmCacheFirst) {\n      testResults.warming = await warmCache();\n      \n      // Wait for cache to settle\n      console.log('‚è≥ Waiting for cache to settle...');\n      await new Promise(resolve => setTimeout(resolve, 3000));\n    }\n    \n    // 3. Test individual endpoint performance\n    console.log('\\nüîç Testing individual endpoint performance...');\n    testResults.endpointTests = [];\n    for (const endpoint of config.endpoints.slice(0, 3)) { // Test first 3 endpoints\n      const endpointResult = await testEndpointPerformance(endpoint, 20);\n      testResults.endpointTests.push(endpointResult);\n      \n      if (endpointResult.estimatedHitRate < config.targetHitRate * 0.5) {\n        console.log(`‚ö†Ô∏è  Low hit rate detected for ${endpoint}`);\n      }\n    }\n    \n    // 4. Run comprehensive benchmark\n    testResults.benchmark = await runCacheBenchmark();\n    \n    // 5. Optimize cache if needed\n    if (testResults.benchmark && testResults.benchmark.summary.overallHitRate < config.targetHitRate) {\n      console.log(`\\n‚ö†Ô∏è  Hit rate ${testResults.benchmark.summary.overallHitRate.toFixed(1)}% is below target ${config.targetHitRate}%`);\n      testResults.optimization = await optimizeCache();\n      \n      // Wait and re-test\n      console.log('‚è≥ Waiting for optimization to take effect...');\n      await new Promise(resolve => setTimeout(resolve, 5000));\n      \n      testResults.postOptimizationBenchmark = await runCacheBenchmark();\n    }\n    \n    // 6. Run load test\n    testResults.loadTest = await runLoadTest();\n    \n    // 7. Get final metrics\n    testResults.finalMetrics = await getCacheMetrics();\n    testResults.finalHealth = await testCacheHealth();\n    \n    // Analyze results\n    console.log('\\nüìä TEST RESULTS SUMMARY');\n    console.log('========================');\n    \n    const finalHitRate = testResults.finalHealth?.hitRate || \n                        testResults.postOptimizationBenchmark?.summary.overallHitRate ||\n                        testResults.benchmark?.summary.overallHitRate || 0;\n                        \n    const finalLatency = testResults.finalHealth?.averageLatency ||\n                        testResults.loadTest?.averageLatency || 0;\n    \n    console.log(`‚úÖ Final cache hit rate: ${finalHitRate.toFixed(1)}%`);\n    console.log(`‚úÖ Final average latency: ${finalLatency.toFixed(1)}ms`);\n    console.log(`‚úÖ Load test throughput: ${testResults.loadTest?.requestsPerSecond.toFixed(1)} requests/second`);\n    \n    // Determine overall success\n    const hitRateSuccess = finalHitRate >= config.targetHitRate;\n    const latencySuccess = finalLatency <= 200; // 200ms threshold\n    const reliabilitySuccess = (testResults.loadTest?.successRate || 0) >= 95;\n    \n    console.log('\\nüéØ TARGET ACHIEVEMENT:');\n    console.log(`   Hit rate ‚â•${config.targetHitRate}%: ${hitRateSuccess ? '‚úÖ' : '‚ùå'} (${finalHitRate.toFixed(1)}%)`);\n    console.log(`   Latency ‚â§200ms: ${latencySuccess ? '‚úÖ' : '‚ùå'} (${finalLatency.toFixed(1)}ms)`);\n    console.log(`   Reliability ‚â•95%: ${reliabilitySuccess ? '‚úÖ' : '‚ùå'} (${(testResults.loadTest?.successRate || 0).toFixed(1)}%)`);\n    \n    overallSuccess = hitRateSuccess && latencySuccess && reliabilitySuccess;\n    \n    console.log(`\\nüèÜ OVERALL RESULT: ${overallSuccess ? '‚úÖ SUCCESS' : '‚ùå NEEDS IMPROVEMENT'}`);\n    \n    if (!overallSuccess) {\n      console.log('\\nüí° RECOMMENDATIONS:');\n      if (!hitRateSuccess) {\n        console.log('   ‚Ä¢ Increase cache warming frequency');\n        console.log('   ‚Ä¢ Adjust TTL values for better retention');\n        console.log('   ‚Ä¢ Add more endpoints to warming strategy');\n      }\n      if (!latencySuccess) {\n        console.log('   ‚Ä¢ Optimize cache key generation');\n        console.log('   ‚Ä¢ Consider using different cache layers');\n        console.log('   ‚Ä¢ Check KV namespace performance');\n      }\n      if (!reliabilitySuccess) {\n        console.log('   ‚Ä¢ Implement better error handling');\n        console.log('   ‚Ä¢ Add circuit breakers for failed requests');\n        console.log('   ‚Ä¢ Improve fallback cache strategies');\n      }\n    }\n    \n  } catch (error) {\n    console.error('\\n‚ùå Test execution failed:', error);\n    overallSuccess = false;\n  }\n  \n  // Exit with appropriate code\n  process.exit(overallSuccess ? 0 : 1);\n}\n\n// Handle command line arguments\nif (process.argv.includes('--help') || process.argv.includes('-h')) {\n  console.log(`\nUsage: node scripts/test-cache-performance.js [options]\n\nOptions:\n  --url <url>          Test URL (default: ${config.baseUrl})\n  --iterations <n>     Test iterations (default: ${config.testIterations})\n  --concurrent <n>     Concurrent requests (default: ${config.concurrentRequests})\n  --duration <ms>      Test duration in ms (default: ${config.testDurationMs})\n  --target <percent>   Target hit rate (default: ${config.targetHitRate})\n  --no-warm           Skip initial cache warming\n  --help, -h          Show this help\n\nEnvironment variables:\n  TEST_URL            Override base URL for testing\n`);\n  process.exit(0);\n}\n\n// Parse command line options\nconst args = process.argv.slice(2);\nfor (let i = 0; i < args.length; i++) {\n  const arg = args[i];\n  if (arg === '--url' && args[i + 1]) {\n    config.baseUrl = args[i + 1];\n    i++;\n  } else if (arg === '--iterations' && args[i + 1]) {\n    config.testIterations = parseInt(args[i + 1]);\n    i++;\n  } else if (arg === '--concurrent' && args[i + 1]) {\n    config.concurrentRequests = parseInt(args[i + 1]);\n    i++;\n  } else if (arg === '--duration' && args[i + 1]) {\n    config.testDurationMs = parseInt(args[i + 1]);\n    i++;\n  } else if (arg === '--target' && args[i + 1]) {\n    config.targetHitRate = parseInt(args[i + 1]);\n    i++;\n  } else if (arg === '--no-warm') {\n    config.warmCacheFirst = false;\n  }\n}\n\n// Set worker start time for uptime calculation\nglobalThis.workerStartTime = Date.now();\n\n// Run the tests\nmain().catch(error => {\n  console.error('\\nüí• Unhandled error:', error);\n  process.exit(1);\n});"