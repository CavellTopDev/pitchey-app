# Chaos Engineering Experiments for Pitchey Platform
# Systematically introduces failures to test system resilience

version: "1.0"

global:
  # Global configuration for all experiments
  defaults:
    duration: "5m"
    steady_state_hypothesis_timeout: "30s"
    rollback_timeout: "60s"
    dry_run: false
    
  # Safety controls
  safety:
    # Only run during specific time windows
    allowed_windows:
      - name: "business_hours_testing"
        days: ["monday", "tuesday", "wednesday", "thursday", "friday"]
        start_time: "10:00"
        end_time: "16:00"
        timezone: "UTC"
      - name: "weekend_extended"
        days: ["saturday", "sunday"]
        start_time: "09:00"
        end_time: "20:00"
        timezone: "UTC"
    
    # Circuit breaker conditions
    abort_conditions:
      - metric: "error_rate"
        threshold: 0.05  # 5% error rate
        duration: "2m"
      - metric: "response_time_p95"
        threshold: 5000  # 5 seconds
        duration: "3m"
      - metric: "active_users"
        threshold: 100   # Minimum active users required
        operator: "less_than"
    
    # Required approvals
    approval_required: true
    approvers: ["sre-team", "platform-team"]
    
    # Notification channels
    notifications:
      before: ["#chaos-engineering", "#sre-alerts"]
      during: ["#chaos-engineering"]
      after: ["#chaos-engineering", "#incident-response"]

# Experiment definitions
experiments:
  
  # Network and Connectivity Experiments
  - name: "database_connection_failure"
    description: "Test resilience when database connections are intermittently failing"
    tags: ["database", "network", "resilience"]
    hypothesis: "The system should gracefully handle database connection failures and continue serving cached content"
    
    steady_state_hypothesis:
      title: "System is healthy before experiment"
      probes:
        - name: "health_check_passes"
          type: "http"
          url: "https://pitchey-production.ndlovucavelle.workers.dev/health"
          expected_status: 200
          timeout: "5s"
        - name: "error_rate_normal"
          type: "prometheus"
          query: "rate(http_requests_total{status=~\"5..\"}[5m]) / rate(http_requests_total[5m])"
          expected_value: 0.02
          operator: "less_than"
        - name: "response_time_normal"
          type: "prometheus"
          query: "histogram_quantile(0.95, rate(http_request_duration_seconds_bucket[5m]))"
          expected_value: 2.0
          operator: "less_than"
    
    method:
      - type: "network_chaos"
        target: "database"
        action: "delay"
        parameters:
          delay: "2s"
          jitter: "500ms"
          probability: 0.3
        duration: "3m"
      - type: "network_chaos"
        target: "database"
        action: "drop_packets"
        parameters:
          probability: 0.1
        duration: "2m"
    
    rollbacks:
      - type: "restore_network"
        target: "database"

  - name: "redis_cache_unavailability"
    description: "Test behavior when Redis cache becomes unavailable"
    tags: ["cache", "redis", "fallback"]
    hypothesis: "Application should fall back to direct database queries when cache is unavailable"
    
    steady_state_hypothesis:
      title: "Cache hit rate is normal"
      probes:
        - name: "cache_hit_rate"
          type: "prometheus"
          query: "cache_hit_rate"
          expected_value: 0.8
          operator: "greater_than"
    
    method:
      - type: "service_failure"
        target: "redis"
        action: "stop_service"
        duration: "4m"
    
    rollbacks:
      - type: "service_recovery"
        target: "redis"
        action: "start_service"

  # Resource Exhaustion Experiments
  - name: "memory_pressure_test"
    description: "Test system behavior under high memory pressure"
    tags: ["memory", "resources", "performance"]
    hypothesis: "System should maintain functionality under memory pressure through garbage collection"
    
    steady_state_hypothesis:
      title: "Memory usage is normal"
      probes:
        - name: "memory_usage_normal"
          type: "prometheus"
          query: "memory_utilization_percent"
          expected_value: 80
          operator: "less_than"
    
    method:
      - type: "resource_stress"
        target: "cloudflare_worker"
        resource: "memory"
        parameters:
          allocation_size: "50MB"
          allocation_rate: "10MB/s"
          duration: "5m"
    
    rollbacks:
      - type: "resource_cleanup"
        target: "cloudflare_worker"

  - name: "cpu_saturation_test"
    description: "Test system response under high CPU load"
    tags: ["cpu", "performance", "throttling"]
    hypothesis: "System should throttle requests gracefully under CPU pressure"
    
    method:
      - type: "resource_stress"
        target: "cloudflare_worker"
        resource: "cpu"
        parameters:
          cpu_percentage: 90
          duration: "3m"

  # Dependency Failure Experiments
  - name: "third_party_api_failure"
    description: "Test behavior when external payment API is down"
    tags: ["external_api", "payment", "circuit_breaker"]
    hypothesis: "Payment failures should not affect other system functionality"
    
    steady_state_hypothesis:
      title: "Payment success rate is normal"
      probes:
        - name: "payment_success_rate"
          type: "prometheus"
          query: "rate(payment_transactions_total{status=\"success\"}[5m]) / rate(payment_transactions_total[5m])"
          expected_value: 0.95
          operator: "greater_than"
    
    method:
      - type: "http_chaos"
        target: "payment_api"
        action: "return_error"
        parameters:
          status_codes: [503, 504, 500]
          probability: 0.8
        duration: "4m"
    
    rollbacks:
      - type: "restore_http"
        target: "payment_api"

  - name: "authentication_service_degradation"
    description: "Test authentication fallback when primary auth service is slow"
    tags: ["authentication", "latency", "fallback"]
    hypothesis: "Auth service should fall back to cached tokens when primary service is slow"
    
    method:
      - type: "latency_injection"
        target: "auth_service"
        parameters:
          delay: "5s"
          jitter: "2s"
          probability: 0.7
        duration: "5m"

  # Data Corruption and Loss Experiments
  - name: "data_consistency_test"
    description: "Test data consistency under concurrent access"
    tags: ["database", "consistency", "concurrency"]
    hypothesis: "Database transactions should maintain ACID properties under high concurrent load"
    
    steady_state_hypothesis:
      title: "No data inconsistencies detected"
      probes:
        - name: "data_integrity_check"
          type: "database_query"
          query: "SELECT COUNT(*) FROM data_integrity_violations WHERE created_at > NOW() - INTERVAL '1 hour'"
          expected_value: 0
    
    method:
      - type: "load_injection"
        target: "database"
        parameters:
          concurrent_connections: 50
          transaction_rate: "100/s"
          duration: "3m"
      - type: "network_partition"
        target: "database_replica"
        duration: "2m"

  # Security and Access Control Experiments
  - name: "jwt_token_expiry_test"
    description: "Test behavior when JWT tokens expire during active sessions"
    tags: ["security", "authentication", "jwt"]
    hypothesis: "System should gracefully handle token expiry and redirect to login"
    
    method:
      - type: "token_manipulation"
        target: "jwt_tokens"
        action: "force_expiry"
        parameters:
          user_percentage: 0.1
        duration: "2m"

  # Infrastructure Failure Experiments
  - name: "cloudflare_edge_failure"
    description: "Simulate failure of Cloudflare edge locations"
    tags: ["cdn", "edge", "failover"]
    hypothesis: "Traffic should automatically route to healthy edge locations"
    
    method:
      - type: "edge_simulation"
        target: "cloudflare_edge"
        action: "block_region"
        parameters:
          regions: ["us-west"]
          probability: 1.0
        duration: "3m"

  - name: "websocket_connection_chaos"
    description: "Test WebSocket resilience under connection instability"
    tags: ["websocket", "real_time", "reconnection"]
    hypothesis: "WebSocket connections should automatically reconnect and restore state"
    
    method:
      - type: "websocket_chaos"
        target: "durable_objects"
        action: "random_disconnect"
        parameters:
          disconnect_probability: 0.3
          reconnect_delay: "1s"
        duration: "4m"

# Experiment schedules
schedules:
  - name: "daily_resilience_testing"
    experiments: 
      - "database_connection_failure"
      - "redis_cache_unavailability"
    cron: "0 14 * * 1-5"  # Weekdays at 2 PM UTC
    enabled: true
    
  - name: "weekly_comprehensive_testing"
    experiments:
      - "database_connection_failure"
      - "redis_cache_unavailability"  
      - "memory_pressure_test"
      - "third_party_api_failure"
      - "jwt_token_expiry_test"
    cron: "0 10 * * 6"  # Saturdays at 10 AM UTC
    enabled: true
    
  - name: "monthly_disaster_simulation"
    experiments:
      - "cloudflare_edge_failure"
      - "data_consistency_test"
      - "cpu_saturation_test"
      - "authentication_service_degradation"
    cron: "0 12 1 * *"  # First day of month at noon UTC
    enabled: true
    approval_required: true

# Game Days - Coordinated chaos engineering events
game_days:
  - name: "payment_system_resilience"
    description: "Test entire payment flow under various failure conditions"
    duration: "2h"
    experiments:
      - "third_party_api_failure"
      - "database_connection_failure"
      - "memory_pressure_test"
    scenarios:
      - name: "black_friday_simulation"
        load_multiplier: 5
        failure_cascade: true
      - name: "payment_provider_outage"
        external_failures: ["payment_api", "fraud_detection_api"]
    
    participants:
      - "sre-team"
      - "platform-team"
      - "payment-team"
      - "security-team"
    
    success_criteria:
      - "Zero payment data loss"
      - "Error rate < 5%"
      - "Recovery time < 10 minutes"
      - "All alerts fire correctly"

# Monitoring and Observability
monitoring:
  # Real-time experiment monitoring
  dashboards:
    - name: "chaos_engineering_overview"
      url: "https://grafana.pitchey.com/d/chaos-overview"
      panels:
        - "Experiment Status"
        - "System Health During Experiments"
        - "Recovery Metrics"
        - "Blast Radius Measurement"
    
    - name: "experiment_impact_analysis"  
      url: "https://grafana.pitchey.com/d/chaos-impact"
      panels:
        - "Error Rate Changes"
        - "Latency Distribution"
        - "User Experience Impact"
        - "Business Metrics"

  # Automated analysis
  analysis:
    - type: "anomaly_detection"
      metric: "error_rate"
      baseline_period: "1h"
      detection_sensitivity: "high"
    
    - type: "correlation_analysis"
      metrics: 
        - "response_time"
        - "error_rate"
        - "user_engagement"
      correlation_threshold: 0.7
    
    - type: "business_impact_assessment"
      metrics:
        - "user_conversion_rate"
        - "revenue_per_minute"
        - "user_satisfaction_score"

# Reporting and Learning
reporting:
  # Automated experiment reports
  post_experiment:
    - type: "executive_summary"
      recipients: ["management@pitchey.com"]
      template: "experiment_summary"
    
    - type: "technical_report"
      recipients: ["engineering@pitchey.com"]
      template: "detailed_analysis"
      include_logs: true
      include_metrics: true
    
    - type: "lessons_learned"
      repository: "chaos_engineering_knowledge_base"
      format: "markdown"

  # Quarterly resilience reports
  quarterly_review:
    enabled: true
    recipients: ["executives@pitchey.com", "board@pitchey.com"]
    include:
      - "System resilience trends"
      - "MTTR improvements"
      - "Incident reduction metrics"
      - "Business continuity validation"

# Integration with incident response
incident_integration:
  # Auto-create incidents for unexpected failures
  auto_incident_creation:
    enabled: true
    severity_threshold: "high"
    assignee: "sre-on-call"
  
  # Use chaos experiments to validate incident response
  incident_validation:
    - name: "payment_outage_response"
      trigger_experiment: "third_party_api_failure"
      validate_runbook: "payment_incident_response"
      success_criteria:
        - "Incident detected within 2 minutes"
        - "Response team assembled within 5 minutes"
        - "Communication sent within 10 minutes"

# Configuration for different environments
environments:
  staging:
    safety:
      abort_conditions:
        - metric: "error_rate"
          threshold: 0.1  # More lenient for staging
    experiments:
      # All experiments allowed in staging
      allowed: ["*"]
  
  production:
    safety:
      abort_conditions:
        - metric: "error_rate"
          threshold: 0.05
        - metric: "active_users"
          threshold: 50
    experiments:
      # Limited experiments in production
      allowed: [
        "database_connection_failure",
        "redis_cache_unavailability", 
        "jwt_token_expiry_test"
      ]
      forbidden: [
        "data_consistency_test",  # Too risky for production
        "cpu_saturation_test"     # Could impact all users
      ]