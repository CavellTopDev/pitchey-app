groups:
  # SRE Golden Signals Alerts
  - name: golden_signals_sre
    interval: 30s
    rules:
      - alert: HighLatency
        expr: histogram_quantile(0.95, sum(rate(latency_seconds_bucket[5m])) by (le)) > 1
        for: 2m
        labels:
          severity: warning
          signal: latency
          team: sre
          runbook_url: "https://docs.pitchey.com/runbooks/high-latency"
        annotations:
          summary: "High latency detected"
          description: "95th percentile latency is {{ $value }}s which is above the 1s threshold"
          impact: "Users experiencing slow response times"
          action_required: "Check performance bottlenecks and scaling"

      - alert: CriticalLatency
        expr: histogram_quantile(0.95, sum(rate(latency_seconds_bucket[5m])) by (le)) > 3
        for: 1m
        labels:
          severity: critical
          signal: latency
          team: sre
          escalate: "true"
        annotations:
          summary: "CRITICAL: Extremely high latency detected"
          description: "95th percentile latency is {{ $value }}s which is critically high"
          impact: "Severe user experience degradation, potential timeouts"
          action_required: "Immediate investigation required - check for outages"

      - alert: HighErrorRate
        expr: sum(rate(http_requests_total{status=~"5.."}[5m])) / sum(rate(http_requests_total[5m])) > 0.05
        for: 2m
        labels:
          severity: warning
          signal: errors
          team: platform
        annotations:
          summary: "High error rate detected"
          description: "Error rate is {{ $value | humanizePercentage }} which is above 5%"
          impact: "Users encountering errors"
          action_required: "Check application logs and recent deployments"

      - alert: CriticalErrorRate
        expr: sum(rate(http_requests_total{status=~"5.."}[5m])) / sum(rate(http_requests_total[5m])) > 0.20
        for: 30s
        labels:
          severity: critical
          signal: errors
          team: platform
          escalate: "true"
        annotations:
          summary: "CRITICAL: Very high error rate"
          description: "Error rate is {{ $value | humanizePercentage }} which is critically high"
          impact: "Major service disruption"
          action_required: "Immediate rollback may be required"

      - alert: TrafficDrop
        expr: sum(rate(http_requests_total[5m])) < 0.1 * sum(rate(http_requests_total[1h] offset 1h))
        for: 5m
        labels:
          severity: warning
          signal: traffic
          team: sre
        annotations:
          summary: "Significant traffic drop detected"
          description: "Traffic is {{ $value }}rps, down {{ $value | humanizePercentage }} from typical levels"
          impact: "Potential service availability issue"
          action_required: "Check for network issues or service outages"

      - alert: HighSaturation
        expr: |
          (
            cpu_utilization_percent > 80 or
            memory_utilization_percent > 85 or
            cache_hit_rate < 70
          )
        for: 3m
        labels:
          severity: warning
          signal: saturation
          team: infrastructure
        annotations:
          summary: "High resource saturation"
          description: "Resource utilization is high - CPU: {{ $value.cpu_utilization_percent }}%, Memory: {{ $value.memory_utilization_percent }}%, Cache Hit Rate: {{ $value.cache_hit_rate }}%"
          impact: "Performance degradation risk"
          action_required: "Consider scaling or optimization"

  # Application Health Alerts
  - name: application_health
    interval: 30s
    rules:
      - alert: ServiceDown
        expr: up == 0
        for: 30s
        labels:
          severity: critical
          team: platform
          escalate: "true"
        annotations:
          summary: "Service {{ $labels.instance }} is down"
          description: "Service {{ $labels.job }}/{{ $labels.instance }} has been down for more than 30 seconds"
          impact: "Service unavailable to users"
          action_required: "Immediate investigation required"

      - alert: HealthCheckFailing
        expr: pitchey_health_score < 70
        for: 2m
        labels:
          severity: warning
          team: platform
        annotations:
          summary: "Health check score is low"
          description: "Overall health score is {{ $value }}% which is below acceptable threshold"
          impact: "Service degradation detected"
          action_required: "Check component health status"

      - alert: DatabaseConnectionFailure
        expr: |
          sum(rate(database_errors_total[5m])) / sum(rate(database_queries_total[5m])) > 0.10
        for: 1m
        labels:
          severity: critical
          team: database
          escalate: "true"
        annotations:
          summary: "High database connection failure rate"
          description: "Database error rate is {{ $value | humanizePercentage }}"
          impact: "Data operations failing"
          action_required: "Check database connectivity and pool status"

      - alert: RedisConnectionFailure
        expr: redis_up == 0
        for: 1m
        labels:
          severity: warning
          team: cache
        annotations:
          summary: "Redis connection failure"
          description: "Redis is not responding"
          impact: "Caching unavailable, performance degradation"
          action_required: "Check Redis cluster health"

  # CI/CD Pipeline Alerts
  - name: cicd_pipeline
    interval: 1m
    rules:
      - alert: PipelineFailureSpike
        expr: increase(github_actions_workflow_runs_total{status="failure"}[30m]) > 5
        for: 0s
        labels:
          severity: warning
          team: platform
        annotations:
          summary: "High number of pipeline failures"
          description: "{{ $value }} pipeline failures in the last 30 minutes"
          impact: "Development velocity impacted"
          action_required: "Check for systemic pipeline issues"

      - alert: DeploymentFailure
        expr: deployment_frequency_total{status="failure"} > 0
        for: 0s
        labels:
          severity: critical
          team: platform
          escalate: "true"
        annotations:
          summary: "Production deployment failed"
          description: "Deployment to {{ $labels.environment }} environment failed"
          impact: "Service update blocked"
          action_required: "Investigate deployment failure and consider rollback"

      - alert: LongPipelineDuration
        expr: github_actions_workflow_duration_seconds > 1800  # 30 minutes
        for: 0s
        labels:
          severity: warning
          team: platform
        annotations:
          summary: "Pipeline duration is unusually long"
          description: "{{ $labels.workflow_name }} took {{ $value | humanizeDuration }}"
          impact: "Development feedback loop delayed"
          action_required: "Investigate pipeline performance bottlenecks"

      - alert: HighSecurityVulnerabilities
        expr: security_vulnerabilities_total{severity="high"} > 0
        for: 0s
        labels:
          severity: warning
          team: security
        annotations:
          summary: "High severity security vulnerabilities found"
          description: "{{ $value }} high severity vulnerabilities in {{ $labels.component }}"
          impact: "Security risk to production"
          action_required: "Review and patch vulnerabilities immediately"

      - alert: CriticalSecurityVulnerabilities
        expr: security_vulnerabilities_total{severity="critical"} > 0
        for: 0s
        labels:
          severity: critical
          team: security
          escalate: "true"
        annotations:
          summary: "CRITICAL security vulnerabilities found"
          description: "{{ $value }} critical vulnerabilities in {{ $labels.component }}"
          impact: "Immediate security threat"
          action_required: "Block deployment and patch immediately"

  # Business Metrics Alerts
  - name: business_metrics
    interval: 5m
    rules:
      - alert: LowUserActivity
        expr: |
          sum(rate(user_actions_total[1h])) < 
          0.5 * avg_over_time(sum(rate(user_actions_total[1h]))[7d:1h] offset 7d)
        for: 30m
        labels:
          severity: warning
          team: product
        annotations:
          summary: "User activity significantly below normal"
          description: "User activity is {{ $value | humanizePercentage }} below typical levels"
          impact: "Potential user experience or technical issue"
          action_required: "Investigate user journey and system health"

      - alert: PaymentProcessingFailure
        expr: |
          sum(rate(payment_transactions_total{status="failed"}[5m])) / 
          sum(rate(payment_transactions_total[5m])) > 0.05
        for: 2m
        labels:
          severity: critical
          team: payments
          escalate: "true"
        annotations:
          summary: "High payment failure rate"
          description: "Payment failure rate is {{ $value | humanizePercentage }}"
          impact: "Revenue loss and customer frustration"
          action_required: "Check payment provider status and logs"

  # Cost Optimization Alerts
  - name: cost_optimization
    interval: 1h
    rules:
      - alert: HighCloudflareUsage
        expr: cloudflare_estimated_cost_usd > 100
        for: 0s
        labels:
          severity: warning
          team: finops
        annotations:
          summary: "High Cloudflare usage detected"
          description: "Daily Cloudflare cost estimate: ${{ $value }}"
          impact: "Increased operational costs"
          action_required: "Review usage patterns and optimize if needed"

      - alert: UnusualResourceConsumption
        expr: |
          (
            increase(cloudflare_requests_total[1h]) > 2 * rate(cloudflare_requests_total[24h] offset 24h) * 24 or
            increase(cloudflare_cpu_time_ms[1h]) > 2 * rate(cloudflare_cpu_time_ms[24h] offset 24h) * 24
          )
        for: 15m
        labels:
          severity: warning
          team: finops
        annotations:
          summary: "Unusual resource consumption pattern"
          description: "Resource usage is significantly above normal patterns"
          impact: "Potential cost spike"
          action_required: "Investigate traffic patterns and potential abuse"

      - alert: BudgetThresholdExceeded
        expr: cloudflare_monthly_cost_usd > 500
        for: 0s
        labels:
          severity: warning
          team: finops
        annotations:
          summary: "Monthly budget threshold exceeded"
          description: "Monthly Cloudflare cost: ${{ $value }}"
          impact: "Budget overrun"
          action_required: "Review and adjust resources or budget"