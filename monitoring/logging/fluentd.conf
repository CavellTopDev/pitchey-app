# Fluentd configuration for Pitchey log aggregation
# Collects logs from multiple sources and routes to destinations

# Input sources
<source>
  @type http
  port 9880
  bind 0.0.0.0
  body_size_limit 32m
  keepalive_timeout 10s
  
  # CORS for browser logs
  cors_allow_origins ["https://pitchey.pages.dev", "https://pitchey-production.cavelltheleaddev.workers.dev"]
  cors_allow_methods ["POST", "OPTIONS"]
  cors_allow_headers ["Content-Type", "Authorization"]
  
  <parse>
    @type json
    time_key timestamp
    time_format %Y-%m-%dT%H:%M:%S.%LZ
  </parse>
  
  tag cloudflare.workers
</source>

# GitHub Actions logs via webhook
<source>
  @type http
  port 9881
  bind 0.0.0.0
  
  <parse>
    @type json
  </parse>
  
  tag github.actions
</source>

# Syslog input for system logs
<source>
  @type syslog
  port 5140
  bind 0.0.0.0
  tag system
</source>

# Forward input for other Fluentd agents
<source>
  @type forward
  port 24224
  bind 0.0.0.0
</source>

# Cloudflare Workers logs processing
<filter cloudflare.workers>
  @type record_transformer
  enable_ruby true
  
  <record>
    # Add standard fields
    log_type "application"
    source "cloudflare-workers"
    environment "#{record['context'] && record['context']['environment'] || 'production'}"
    service_name "#{record['context'] && record['context']['service'] || 'pitchey-api'}"
    
    # Extract user information
    user_id "#{record['context'] && record['context']['userId']}"
    session_id "#{record['context'] && record['context']['sessionId']}"
    
    # Extract performance metrics
    response_time_ms "#{record['performance'] && record['performance']['duration_ms']}"
    memory_mb "#{record['performance'] && record['performance']['memory_mb']}"
    
    # Extract error information
    error_type "#{record['error'] && record['error']['type']}"
    error_fingerprint "#{record['error'] && record['error']['fingerprint']}"
    
    # Extract business metrics
    business_event "#{record['business'] && record['business']['event_type']}"
    business_value "#{record['business'] && record['business']['value']}"
    
    # Security events
    security_event "#{record['security'] && record['security']['event_type']}"
    risk_score "#{record['security'] && record['security']['risk_score']}"
  </record>
</filter>

# GitHub Actions logs processing
<filter github.actions>
  @type record_transformer
  enable_ruby true
  
  <record>
    log_type "cicd"
    source "github-actions"
    environment "#{record['environment'] || 'ci'}"
    
    # Workflow information
    workflow_name "#{record['workflow']}"
    workflow_status "#{record['conclusion']}"
    workflow_duration "#{record['duration_ms']}"
    
    # Repository information
    repository "#{record['repository']}"
    branch "#{record['head_branch']}"
    commit_sha "#{record['head_sha']}"
    actor "#{record['actor']}"
    
    # Job information
    job_name "#{record['job']}"
    step_name "#{record['step']}"
  </record>
</filter>

# System logs processing
<filter system>
  @type record_transformer
  
  <record>
    log_type "system"
    source "syslog"
    environment "production"
  </record>
</filter>

# Enrich logs with geo-location for security analysis
<filter **>
  @type geoip
  geoip_lookup_keys ip_address
  
  <record>
    geo_country "#{record['geo']['country']}"
    geo_city "#{record['geo']['city']}"
    geo_latitude "#{record['geo']['latitude']}"
    geo_longitude "#{record['geo']['longitude']}"
  </record>
  
  skip_adding_null_record true
</filter>

# Add hostname and timestamp normalization
<filter **>
  @type record_transformer
  enable_ruby true
  
  <record>
    hostname "#{Socket.gethostname}"
    normalized_timestamp "#{Time.now.utc.iso8601(3)}"
    
    # Calculate log age for monitoring
    log_age_seconds "#{Time.now.to_f - Time.parse(record['timestamp']).to_f rescue 0}"
  </record>
</filter>

# Parse and extract structured data from message field
<filter **>
  @type parser
  key_name message
  reserve_data true
  inject_key_prefix extracted_
  
  <parse>
    @type multi_format
    
    # JSON logs
    <pattern>
      format json
    </pattern>
    
    # Common log format
    <pattern>
      format regexp
      expression /^(?<host>\S+) \S+ \S+ \[(?<time>[^\]]+)\] "(?<method>\S+) (?<path>\S+) HTTP\/[\d\.]+\" (?<status>\d+) (?<size>\d+)/
      time_format %d/%b/%Y:%H:%M:%S %z
    </pattern>
    
    # Default fallback
    <pattern>
      format none
    </pattern>
  </parse>
</filter>

# Error detection and alerting
<filter **>
  @type grep
  <regexp>
    key level
    pattern ^(error|fatal|critical)$
  </regexp>
  
  <or>
    <regexp>
      key status_code
      pattern ^5\d\d$
    </regexp>
  </or>
</filter>

# Rate limiting and sampling for high-volume logs
<filter cloudflare.workers>
  @type sampling
  sample_rate 0.1
  interval 1
  
  # Always sample errors and critical events
  <rule>
    sample_rate 1.0
    <condition>
      key level
      pattern ^(error|fatal|warn)$
    </condition>
  </rule>
  
  <rule>
    sample_rate 1.0
    <condition>
      key security_event
      pattern .+
    </condition>
  </rule>
</filter>

# Output to Elasticsearch for search and analytics
<match **>
  @type copy
  
  # Primary Elasticsearch cluster
  <store>
    @type elasticsearch
    host elasticsearch-primary.internal
    port 9200
    user "#{ENV['ELASTICSEARCH_USERNAME']}"
    password "#{ENV['ELASTICSEARCH_PASSWORD']}"
    
    # Index configuration
    index_name pitchey-logs
    template_name pitchey-logs-template
    
    # Index lifecycle management
    ilm_enabled true
    ilm_policy_id pitchey-logs-policy
    
    # Document type
    type_name _doc
    
    # Buffer configuration for performance
    <buffer tag,time>
      @type file
      path /var/log/fluentd/buffer/elasticsearch
      timekey 60s
      timekey_use_utc true
      chunk_limit_size 32m
      total_limit_size 1g
      flush_mode interval
      flush_interval 10s
      retry_max_interval 30s
      retry_forever true
    </buffer>
    
    # Template for index mapping
    template_file /etc/fluentd/templates/pitchey-logs-template.json
    template_overwrite true
  </store>
  
  # Backup to secondary Elasticsearch cluster
  <store>
    @type elasticsearch
    host elasticsearch-backup.internal
    port 9200
    user "#{ENV['ELASTICSEARCH_BACKUP_USERNAME']}"
    password "#{ENV['ELASTICSEARCH_BACKUP_PASSWORD']}"
    
    index_name pitchey-logs-backup
    
    <buffer tag,time>
      @type file
      path /var/log/fluentd/buffer/elasticsearch-backup
      timekey 300s
      chunk_limit_size 64m
      flush_mode interval
      flush_interval 60s
    </buffer>
  </store>
  
  # Send to Grafana Loki for metrics and alerting
  <store>
    @type loki
    url http://loki.internal:3100
    username "#{ENV['LOKI_USERNAME']}"
    password "#{ENV['LOKI_PASSWORD']}"
    
    # Labels for Loki streams
    <label>
      level
      service_name
      environment
      log_type
    </label>
    
    <buffer>
      @type file
      path /var/log/fluentd/buffer/loki
      flush_mode interval
      flush_interval 5s
      chunk_limit_size 1m
    </buffer>
  </store>
  
  # Send high-priority alerts to webhook
  <store>
    @type copy
    
    # Critical errors to PagerDuty
    <store>
      @type http
      endpoint_url "#{ENV['PAGERDUTY_WEBHOOK_URL']}"
      http_method post
      
      headers {"Content-Type": "application/json"}
      
      <format>
        @type json
      </format>
      
      <buffer>
        @type memory
        flush_mode immediate
        retry_max_interval 10s
      </buffer>
      
      # Only send critical events
      <filter>
        @type grep
        <regexp>
          key level
          pattern ^(fatal|critical)$
        </regexp>
        
        <or>
          <regexp>
            key error_type
            pattern ^(DatabaseConnectionError|PaymentProcessingError|SecurityBreach)$
          </regexp>
        </or>
      </filter>
    </store>
    
    # Security events to security webhook
    <store>
      @type http
      endpoint_url "#{ENV['SECURITY_WEBHOOK_URL']}"
      http_method post
      
      <format>
        @type json
      </format>
      
      # Only send security events
      <filter>
        @type grep
        <regexp>
          key security_event
          pattern .+
        </regexp>
        
        <or>
          <regexp>
            key risk_score
            pattern ^[8-9]\d*$|^100$  # Risk score 80+
          </regexp>
        </or>
      </filter>
    </store>
  </store>
  
  # Archive to S3 for long-term storage
  <store>
    @type s3
    aws_key_id "#{ENV['AWS_ACCESS_KEY_ID']}"
    aws_sec_key "#{ENV['AWS_SECRET_ACCESS_KEY']}"
    s3_bucket pitchey-logs-archive
    s3_region us-west-2
    
    # Path format for organization
    path logs/year=%Y/month=%m/day=%d/hour=%H/
    
    # Compression and format
    <format>
      @type json
    </format>
    store_as gzip_command
    
    # Buffer for batching
    <buffer time>
      @type file
      path /var/log/fluentd/buffer/s3
      timekey 3600s  # Hourly files
      chunk_limit_size 256m
      flush_mode interval
      flush_interval 300s
    </buffer>
  </store>
  
  # Send metrics to StatsD for real-time monitoring
  <store>
    @type statsd
    host statsd.internal
    port 8125
    
    # Convert log events to metrics
    <metric>
      type counter
      name log.count
      tag level,service_name,environment
    </metric>
    
    <metric>
      type histogram
      name log.response_time
      key response_time_ms
      tag service_name,environment
    </metric>
    
    <metric>
      type counter
      name log.errors
      key error_type
      tag service_name,environment
      
      <filter>
        @type grep
        <regexp>
          key level
          pattern ^(error|fatal)$
        </regexp>
      </filter>
    </metric>
  </store>
</match>

# Dead letter queue for failed log deliveries
<match **>
  @type file
  path /var/log/fluentd/failed-logs/failed.%Y%m%d
  append true
  
  <buffer time>
    timekey 86400s  # Daily files
    timekey_use_utc true
  </buffer>
</match>